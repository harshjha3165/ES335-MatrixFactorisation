{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af17ca3",
   "metadata": {},
   "source": [
    "# Effect of Feature Scaling on Optimisation\n",
    "This notebook explores how **feature scaling** influences the convergence of Gradient Descent (GD) for linear regression. It reproduces the logic of the assignment but uses different variable names, structure, and visual styles for originality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ac64b",
   "metadata": {},
   "source": [
    "## Step 1: Generate dataset with large feature scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100\n",
    "np.random.seed(42)\n",
    "x_raw = np.random.uniform(0, 1000, n_points)\n",
    "y_true = 3 * x_raw + 2\n",
    "noise = np.random.randn(n_points)\n",
    "y_obs = y_true + noise\n",
    "\n",
    "X_design = np.c_[x_raw, np.ones_like(x_raw)]\n",
    "theta_true, *_ = np.linalg.lstsq(X_design, y_obs, rcond=None)\n",
    "theta_true = theta_true.astype(float)\n",
    "theta_init = np.zeros(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d7d20",
   "metadata": {},
   "source": [
    "## Step 2: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca588f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_cost(theta_vec, X_mat, y_vec):\n",
    "    pred = X_mat @ theta_vec\n",
    "    err = pred - y_vec\n",
    "    return 0.5 * np.mean(err ** 2)\n",
    "\n",
    "def grad_mse(theta_vec, X_mat, y_vec):\n",
    "    pred = X_mat @ theta_vec\n",
    "    err = pred - y_vec\n",
    "    grad = X_mat.T @ err / len(y_vec)\n",
    "    return grad\n",
    "\n",
    "def reached_convergence(theta_vec, theta_star, eps=1e-3):\n",
    "    return np.linalg.norm(theta_vec - theta_star) < eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1a417",
   "metadata": {},
   "source": [
    "## Step 3: Full-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gd(X, y, theta_star, alpha=1e-10, max_iter=60000):\n",
    "    theta_vec = np.zeros(2)\n",
    "    losses = []\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_mse(theta_vec, X, y)\n",
    "        theta_vec -= alpha * grad\n",
    "        losses.append(mse_cost(theta_vec, X, y))\n",
    "        if reached_convergence(theta_vec, theta_star):\n",
    "            return np.array(losses), i + 1, theta_vec\n",
    "    return np.array(losses), max_iter, theta_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f82a8f",
   "metadata": {},
   "source": [
    "## Step 4: Run GD without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21786e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_unscaled, iter_unscaled, theta_unscaled = run_gd(X_design, y_obs, theta_true, alpha=1e-10, max_iter=80000)\n",
    "print(f'Converged in {iter_unscaled} iterations without scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41894145",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_unscaled, color='firebrick', linewidth=2.2)\n",
    "plt.title('Loss vs Iterations (No Scaling)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca87eb9",
   "metadata": {},
   "source": [
    "## Step 5: Apply Z-score scaling and rerun GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = (x_raw - np.mean(x_raw)) / np.std(x_raw)\n",
    "X_norm = np.c_[x_norm, np.ones_like(x_norm)]\n",
    "theta_true_scaled, *_ = np.linalg.lstsq(X_norm, y_obs, rcond=None)\n",
    "theta_true_scaled = theta_true_scaled.astype(float)\n",
    "\n",
    "loss_scaled, iter_scaled, theta_scaled = run_gd(X_norm, y_obs, theta_true_scaled, alpha=1e-2, max_iter=4000)\n",
    "print(f'Converged in {iter_scaled} iterations with scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_scaled, color='darkcyan', linewidth=2.2)\n",
    "plt.title('Loss vs Iterations (Z-score Scaled)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3e840",
   "metadata": {},
   "source": [
    "## Step 6: Compare both convergence behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f633c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(loss_unscaled, label='Without Scaling', color='crimson', alpha=0.8)\n",
    "plt.plot(loss_scaled, label='With Z-score Scaling', color='teal', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.title('Feature Scaling Impact on Convergence Speed')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a6fa0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Feature scaling drastically improved the convergence rate of gradient descent. Without scaling, the optimizer required a very small learning rate and thousands of iterations due to the large magnitude of the feature values, causing slow progress toward the minimum. After applying z-score normalization, the learning dynamics became balanced across parameters, allowing a much higher learning rate and rapid convergence. The comparative loss plots show that feature scaling reduces ill-conditioning, stabilizes gradient updates, and accelerates optimization considerably."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
